{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf3eb8ff-6e43-45d0-b9f0-98cd0b18f80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import json\n",
    "\n",
    "import minio\n",
    "from minio import Minio\n",
    "from pymongo import MongoClient\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d598139b-6925-4f80-b421-1c78495916ee",
   "metadata": {},
   "source": [
    "# Dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f987b56-241e-4b72-958b-e75436fd979b",
   "metadata": {},
   "source": [
    "Przykładowy rekord."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e052597f-9432-436f-88b4-ba0026ec1acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = open(\"../../data/arxiv-metadata-oai-snapshot.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b23ed26-ff0a-480f-974e-8556d8d88c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '0704.0001', 'submitter': 'Pavel Nadolsky', 'authors': \"C. Bal\\\\'azs, E. L. Berger, P. M. Nadolsky, C.-P. Yuan\", 'title': 'Calculation of prompt diphoton production cross sections at Tevatron and\\n  LHC energies', 'comments': '37 pages, 15 figures; published version', 'journal-ref': 'Phys.Rev.D76:013009,2007', 'doi': '10.1103/PhysRevD.76.013009', 'report-no': 'ANL-HEP-PR-07-12', 'categories': 'hep-ph', 'license': None, 'abstract': '  A fully differential calculation in perturbative quantum chromodynamics is\\npresented for the production of massive photon pairs at hadron colliders. All\\nnext-to-leading order perturbative contributions from quark-antiquark,\\ngluon-(anti)quark, and gluon-gluon subprocesses are included, as well as\\nall-orders resummation of initial-state gluon radiation valid at\\nnext-to-next-to-leading logarithmic accuracy. The region of phase space is\\nspecified in which the calculation is most reliable. Good agreement is\\ndemonstrated with data from the Fermilab Tevatron, and predictions are made for\\nmore detailed tests with CDF and DO data. Predictions are shown for\\ndistributions of diphoton pairs produced at the energy of the Large Hadron\\nCollider (LHC). Distributions of the diphoton pairs from the decay of a Higgs\\nboson are contrasted with those produced from QCD processes at the LHC, showing\\nthat enhanced sensitivity to the signal can be obtained with judicious\\nselection of events.\\n', 'versions': [{'version': 'v1', 'created': 'Mon, 2 Apr 2007 19:18:42 GMT'}, {'version': 'v2', 'created': 'Tue, 24 Jul 2007 20:10:27 GMT'}], 'update_date': '2008-11-26', 'authors_parsed': [['Balázs', 'C.', ''], ['Berger', 'E. L.', ''], ['Nadolsky', 'P. M.', ''], ['Yuan', 'C. -P.', '']]}\n"
     ]
    }
   ],
   "source": [
    "example = json.loads(next(dataset))\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b417418-c2ea-4588-b7c1-646fe78b4812",
   "metadata": {},
   "source": [
    "## MongoDB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc53873-f6b6-4075-9a18-9088d3da9b96",
   "metadata": {},
   "source": [
    "Połącz się z MongoDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5955b8a8-e186-4cf0-8366-9edcc1189138",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient(\"mongodb://root:example@mongo:27017/admin\")\n",
    "db = client[\"arxiv\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e6a1f6-3136-4436-b779-f401514de7c3",
   "metadata": {},
   "source": [
    "Sprawdź czy baza danych została utworzona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "391ffda0-68cc-492b-8157-dd9c24dfb6af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['admin', 'arxiv', 'config', 'local']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.list_database_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95684fce-9df7-4121-8eab-d1806bdae1b0",
   "metadata": {},
   "source": [
    "Utwórz kolekcję."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97561b48-3b84-46cc-9123-570d5109e465",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = db[\"papers\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dd0988-041b-4810-8970-88640e1c2265",
   "metadata": {},
   "source": [
    "Wstaw przykładowy rekord danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21b56283-d70a-4bb7-a2f2-80dbc6d6c6af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InsertOneResult(ObjectId('6652e92b341cf052182941c8'), acknowledged=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.insert_one(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0abf2e-3358-4084-a712-1aa47c09782e",
   "metadata": {},
   "source": [
    "Sprawdź czy baza danych została utworzona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b1f31b5-5424-4456-b376-6896730c9d79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['admin', 'arxiv', 'config', 'local']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.list_database_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6f1ea2-8719-4b07-bd69-28f443a7904e",
   "metadata": {},
   "source": [
    "Konfiguracja sesji Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01550101-d3c2-44e2-982e-91e1a0eefe67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/26 09:47:56 WARN Utils: Your hostname, MacBook.local resolves to a loopback address: 127.0.0.1; using 192.168.0.193 instead (on interface en0)\n",
      "24/05/26 09:47:56 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /Users/lukasz/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/lukasz/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-141bbaa1-d1dd-4313-8e5f-0529d733ffea;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector;10.0.1 in central\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/lukasz/Documents/Dokumenty%20%e2%80%94%20MacBook%20Air%20(Lukasz)/edu/pw-big-data-thesis/venv/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.mongodb#mongodb-driver-sync;4.5.1 in central\n",
      "\t[4.5.1] org.mongodb#mongodb-driver-sync;[4.5.0,4.5.99)\n",
      "\tfound org.mongodb#bson;4.5.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.5.1 in central\n",
      ":: resolution report :: resolve 2093ms :: artifacts dl 4ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#bson;4.5.1 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.5.1 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.5.1 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector;10.0.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   1   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-141bbaa1-d1dd-4313-8e5f-0529d733ffea\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/4ms)\n",
      "24/05/26 09:47:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.appName(\"read-json-from-mongo\")\n",
    "    .master(\"spark://spark:7077\")\n",
    "    .config(\n",
    "        \"spark.mongodb.read.connection.uri\",\n",
    "        \"mongodb://root:example@mongo:27017/admin\",\n",
    "    )\n",
    "    .config(\"spark.mongodb.read.database\", \"arxiv\")\n",
    "    .config(\"spark.mongodb.read.collection\", \"papers\")\n",
    "    .config(\n",
    "        \"spark.mongodb.write.connection.uri\",\n",
    "        \"mongodb://root:example@mongo:27017/admin/arxiv.papers\",\n",
    "    )\n",
    "    .config(\"spark.mongodb.write.database\", \"arxiv\")\n",
    "    .config(\"spark.mongodb.read.collection\", \"papers\")\n",
    "    .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector:10.0.1\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73c1ab78-b82e-4d8a-84df-0ec1c9f584c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.193:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>read-json-from-mongo</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1064a69f0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a49da9c2-6c5f-4743-9fbe-3e72d20652f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"mongodb\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a33d5d04-6ab5-42e1-b6ff-17fc20bb8ef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_id: string, abstract: string, authors: string, authors_parsed: array<array<string>>, categories: string, comments: string, doi: string, id: string, journal-ref: string, license: void, report-no: string, submitter: string, title: string, update_date: string, versions: array<struct<version:string,created:string>>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f652ccbc-b6e2-461c-8396-2c2f49d94dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5105b1-4639-415a-ab9f-c85460f1ab8f",
   "metadata": {},
   "source": [
    "# Filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32ae627a-1640-4252-a558-845f0875ae11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "import pyspark.sql.functions as F\n",
    "from dotenv import dotenv_values\n",
    "from transformers import pipeline\n",
    "\n",
    "from rag.clients import setup_spark_session\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "LOGGER.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19432397-216c-430f-81e2-5ca6db72c99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Setting up Spark...\n",
      "INFO:__main__:Reading data from MongoDB...\n"
     ]
    }
   ],
   "source": [
    "LOGGER.info(\"Setting up Spark...\")\n",
    "spark = setup_spark_session()\n",
    "LOGGER.info(\"Reading data from MongoDB...\")\n",
    "ENV = dict()\n",
    "source = \"s3a://papers/100k\"\n",
    "data = (\n",
    "    spark.read.format(\"mongodb\")\n",
    "    .option(\"database\", ENV.get(\"MONGO_DATABASE\", \"arxiv\"))\n",
    "    .option(\"collection\", ENV.get(\"MONGO_COLLECTION\", \"sentences\"))\n",
    "    .option(\"pipeline\", f\"{{ $match: {{ source: '{source}' }} }}\")\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa8265af-81e0-43a1-a461-5a9e520e29c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+----------------+\n",
      "|      _id|           full_text|           sentences|          source|\n",
      "+---------+--------------------+--------------------+----------------+\n",
      "|0704.0001|  A fully differe...|[A fully differen...|s3a:/papers/100k|\n",
      "|0704.0002|  We describe a n...|[We describe a ne...|s3a:/papers/100k|\n",
      "|0704.0003|  The evolution o...|[The evolution of...|s3a:/papers/100k|\n",
      "|0704.0004|  We show that a ...|[We show that a d...|s3a:/papers/100k|\n",
      "|0704.0005|  In this paper w...|[In this paper we...|s3a:/papers/100k|\n",
      "|0704.0006|  We study the tw...|[We study the two...|s3a:/papers/100k|\n",
      "|0704.0007|  A rather non-st...|[A rather non-sta...|s3a:/papers/100k|\n",
      "|0704.0008|  A general formu...|[A general formul...|s3a:/papers/100k|\n",
      "|0704.0009|  We discuss the ...|[We discuss the r...|s3a:/papers/100k|\n",
      "|0704.0010|  Partial cubes a...|[Partial cubes ar...|s3a:/papers/100k|\n",
      "+---------+--------------------+--------------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b291bbf-f7b3-4cdf-be77-31a3e6a80e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "200000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
