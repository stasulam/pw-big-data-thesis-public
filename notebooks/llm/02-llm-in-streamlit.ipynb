{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76473514-2086-40cf-8526-6e66c2992e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 24 key-value pairs and 195 tensors from ../../models/Phi-3-mini-4k-instruct-q4.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = phi3\n",
      "llama_model_loader: - kv   1:                               general.name str              = Phi3\n",
      "llama_model_loader: - kv   2:                        phi3.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                      phi3.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv   4:                   phi3.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv   5:                           phi3.block_count u32              = 32\n",
      "llama_model_loader: - kv   6:                  phi3.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:               phi3.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   8:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv   9:                  phi3.rope.dimension_count u32              = 96\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32064]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32064]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32064]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 32000\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:   81 tensors\n",
      "llama_model_loader: - type q5_K:   32 tensors\n",
      "llama_model_loader: - type q6_K:   17 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 323/32064 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = phi3\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32064\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 3072\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 96\n",
      "llm_load_print_meta: n_embd_head_k    = 96\n",
      "llm_load_print_meta: n_embd_head_v    = 96\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 3072\n",
      "llm_load_print_meta: n_embd_v_gqa     = 3072\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 8192\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 3B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 3.82 B\n",
      "llm_load_print_meta: model size       = 2.23 GiB (5.01 BPW) \n",
      "llm_load_print_meta: general.name     = Phi3\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 32000 '<|endoftext|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 32000 '<|endoftext|>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 32007 '<|end|>'\n",
      "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
      "ggml_backend_metal_log_allocated_size: allocated buffer, size =  2228.84 MiB, ( 2228.91 / 10922.67)\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =    52.84 MiB\n",
      "llm_load_tensors:      Metal buffer size =  2228.83 MiB\n",
      "...........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2\n",
      "ggml_metal_init: picking default device: Apple M2\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =  1536.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   300.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    14.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1286\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') %}{{'<|user|>' + '\\n' + message['content'] + '<|end|>' + '\\n' + '<|assistant|>' + '\\n'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '<|end|>' + '\\n'}}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.padding_token_id': '32000', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '32000', 'tokenizer.ggml.pre': 'default', 'general.file_type': '15', 'general.quantization_version': '2', 'phi3.rope.dimension_count': '96', 'tokenizer.ggml.bos_token_id': '1', 'phi3.attention.layer_norm_rms_epsilon': '0.000010', 'phi3.attention.head_count_kv': '32', 'phi3.attention.head_count': '32', 'tokenizer.ggml.model': 'llama', 'phi3.block_count': '32', 'general.architecture': 'phi3', 'phi3.feed_forward_length': '8192', 'phi3.embedding_length': '3072', 'general.name': 'Phi3', 'phi3.context_length': '4096'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') %}{{'<|user|>' + '\n",
      "' + message['content'] + '<|end|>' + '\n",
      "' + '<|assistant|>' + '\n",
      "'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '<|end|>' + '\n",
      "'}}{% endif %}{% endfor %}\n",
      "Using chat eos_token: <|endoftext|>\n",
      "Using chat bos_token: <s>\n"
     ]
    }
   ],
   "source": [
    "\"\"\"LLM.\"\"\"\n",
    "\n",
    "import streamlit as st\n",
    "from llama_cpp import Llama\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from webapp.Hello import data, full_df\n",
    "\n",
    "\n",
    "MODEL_PATH = \"../../models/Phi-3-mini-4k-instruct-q4.gguf\"\n",
    "\n",
    "MODEL = Llama(\n",
    "    model_path=MODEL_PATH,\n",
    "    n_ctx=4096,  # The max sequence length to use - note that longer sequence lengths require much more resources\n",
    "    n_threads=8,  # The number of CPU threads to use, tailor to your system and the resulting performance\n",
    "    n_gpu_layers=64,  # The number of layers to offload to GPU, if you have GPU acceleration available. Set to 0 if no GPU acceleration is available on your system.\n",
    ")\n",
    "\n",
    "PROMPT = \"\"\"\n",
    "Context: ```\n",
    "{context}\n",
    "```\n",
    "Given the context inside ``` solve the following taks: {task}.\n",
    "If the context is not enough, try to solve the task with the\n",
    "knowledge you have. But inform the user that the context is not\n",
    "enough to solve the task.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_most_similar_docs(text: str, num_docs: int) -> str:\n",
    "    \"\"\"Get most similar documents.\"\"\"\n",
    "    from rag.processor.most_similar_docs import get_most_similar_documents\n",
    "\n",
    "    ids = get_most_similar_documents(\n",
    "        text=text,\n",
    "        data=data,\n",
    "        num_docs=num_docs,\n",
    "    )\n",
    "    texts = full_df.filter(F.col(\"_id\").isin(ids)).select(\"full_text\").collect()\n",
    "    context = \"\\n\".join([text[\"full_text\"] for text in texts])\n",
    "    return ids, context\n",
    "\n",
    "\n",
    "def qa(text: str, num_docs: int = 3) -> str:\n",
    "    \"\"\"Question & Answers.\"\"\"\n",
    "    ids, context = get_most_similar_docs(text=text, num_docs=num_docs)\n",
    "    prompt = PROMPT.format(context=context, task=text)\n",
    "    output = MODEL(\n",
    "        f\"<|user|>\\n{prompt}<|end|>\\n<|assistant|>\",\n",
    "        max_tokens=256,  # Generate up to 256 tokens\n",
    "        stop=[\"<|end|>\"],\n",
    "        echo=False,  # Whether to echo the prompt\n",
    "    )\n",
    "    return ids, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6e107ce-bb2f-4488-87de-b93d3a3c4ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lukasz/Documents/pw-big-data-thesis/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:rag.processor.most_similar_docs:Processing query...\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: mps\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
      "/Users/lukasz/Documents/pw-big-data-thesis/venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Batches: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.85it/s]\n",
      "INFO:rag.processor.most_similar_docs:Calculating cosine similarity...\n",
      "INFO:rag.processor.most_similar_docs:Number of obs: 2402                        \n",
      "INFO:rag.processor.most_similar_docs:Getting most similar documents...\n",
      "INFO:rag.processor.most_similar_docs:Most similar documents:                    \n",
      "INFO:rag.processor.most_similar_docs:['0704.0322', '0704.0114', '0704.0310']\n",
      "\n",
      "llama_print_timings:        load time =    8632.89 ms\n",
      "llama_print_timings:      sample time =      21.82 ms /   256 runs   (    0.09 ms per token, 11729.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11546.49 ms /  1110 tokens (   10.40 ms per token,    96.13 tokens per second)\n",
      "llama_print_timings:        eval time =   12766.50 ms /   255 runs   (   50.06 ms per token,    19.97 tokens per second)\n",
      "llama_print_timings:       total time =   24450.02 ms /  1365 tokens\n"
     ]
    }
   ],
   "source": [
    "ids, answer = qa(\"What is phytoplankton?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "331c77a3-4726-45ae-949c-7f823e37281c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream(output):\n",
    "    for token in output[\"choices\"][0][\"text\"].split(\" \"):\n",
    "        yield token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed451d6c-844f-4920-a930-53859748c65f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Phytoplankton are a diverse collection of microscopic organisms found in aquatic environments, both saline and freshwater. They are autotrophs, meaning they produce their own food through photosynthesis by converting sunlight, carbon dioxide, and water into glucose and oxygen. This makes phytoplankton the primary producers in aquatic ecosystems, forming a fundamental base for most marine and freshwater food webs. They are crucial not only because they contribute significantly to the global oxygen production but also play a pivotal role in carbon cycling by absorbing CO2 from the atmosphere.\\n\\nGiven the context provided, phytoplankton are not directly mentioned. However, based on general knowledge outside of this specific text, we can understand that phytoplankton were likely discussed within the broader framework of their role in reaction-diffusion models and how these organisms might exhibit various behaviors (including regular, chaotic behavior, and spatiotemporal patterns) under different environmental conditions. These studies are crucial for understanding ecological dynamics, population distribution, and interactions among species within aquatic environments.\\n\\nThe context provided'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(stream(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fb2374c-6b0f-453b-a8f4-954be7ebfef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Phytoplankton\n",
      "are\n",
      "a\n",
      "diverse\n",
      "collection\n",
      "of\n",
      "microscopic\n",
      "organisms\n",
      "found\n",
      "in\n",
      "aquatic\n",
      "environments,\n",
      "both\n",
      "saline\n",
      "and\n",
      "freshwater.\n",
      "They\n",
      "are\n",
      "autotrophs,\n",
      "meaning\n",
      "they\n",
      "produce\n",
      "their\n",
      "own\n",
      "food\n",
      "through\n",
      "photosynthesis\n",
      "by\n",
      "converting\n",
      "sunlight,\n",
      "carbon\n",
      "dioxide,\n",
      "and\n",
      "water\n",
      "into\n",
      "glucose\n",
      "and\n",
      "oxygen.\n",
      "This\n",
      "makes\n",
      "phytoplankton\n",
      "the\n",
      "primary\n",
      "producers\n",
      "in\n",
      "aquatic\n",
      "ecosystems,\n",
      "forming\n",
      "a\n",
      "fundamental\n",
      "base\n",
      "for\n",
      "most\n",
      "marine\n",
      "and\n",
      "freshwater\n",
      "food\n",
      "webs.\n",
      "They\n",
      "are\n",
      "crucial\n",
      "not\n",
      "only\n",
      "because\n",
      "they\n",
      "contribute\n",
      "significantly\n",
      "to\n",
      "the\n",
      "global\n",
      "oxygen\n",
      "production\n",
      "but\n",
      "also\n",
      "play\n",
      "a\n",
      "pivotal\n",
      "role\n",
      "in\n",
      "carbon\n",
      "cycling\n",
      "by\n",
      "absorbing\n",
      "CO2\n",
      "from\n",
      "the\n",
      "atmosphere.\n",
      "\n",
      "Given\n",
      "the\n",
      "context\n",
      "provided,\n",
      "phytoplankton\n",
      "are\n",
      "not\n",
      "directly\n",
      "mentioned.\n",
      "However,\n",
      "based\n",
      "on\n",
      "general\n",
      "knowledge\n",
      "outside\n",
      "of\n",
      "this\n",
      "specific\n",
      "text,\n",
      "we\n",
      "can\n",
      "understand\n",
      "that\n",
      "phytoplankton\n",
      "were\n",
      "likely\n",
      "discussed\n",
      "within\n",
      "the\n",
      "broader\n",
      "framework\n",
      "of\n",
      "their\n",
      "role\n",
      "in\n",
      "reaction-diffusion\n",
      "models\n",
      "and\n",
      "how\n",
      "these\n",
      "organisms\n",
      "might\n",
      "exhibit\n",
      "various\n",
      "behaviors\n",
      "(including\n",
      "regular,\n",
      "chaotic\n",
      "behavior,\n",
      "and\n",
      "spatiotemporal\n",
      "patterns)\n",
      "under\n",
      "different\n",
      "environmental\n",
      "conditions.\n",
      "These\n",
      "studies\n",
      "are\n",
      "crucial\n",
      "for\n",
      "understanding\n",
      "ecological\n",
      "dynamics,\n",
      "population\n",
      "distribution,\n",
      "and\n",
      "interactions\n",
      "among\n",
      "species\n",
      "within\n",
      "aquatic\n",
      "environments.\n",
      "\n",
      "The\n",
      "context\n",
      "provided\n"
     ]
    }
   ],
   "source": [
    "for word in answer[\"choices\"][0][\"text\"].split(\" \"):\n",
    "    print(word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
